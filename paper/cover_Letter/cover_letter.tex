Firstly, we would like to thank the reviewers and the meta reviewer for their insight and valuable comments which enable us to greatly improve the quality of our manuscript. We have carefully taken your comments into consideration in preparing this revision.
Below we summarize the major changes in each section in \refsec{modifications} and provide the detailed response to the feedbacks in \refsec{response}.


\section{Major Modifications in this Revision}
\label{sec:modifications}

\textbf{In Section 1, we mainly make the following modifications}:
\begin{itemize}
	\item We have made the query example on Page 1 (in the original manuscript) a figure float. (R3-M1)
	\item We have removed \rgmapping from the contributions of this paper. (R2-i.6, R3-O1.1)
\end{itemize}

\textbf{In Section 2, we mainly make the following modifications}:
\begin{itemize}
	\item We move Table 1 earlier to make the notations in this section easier to understand. (R3-M1)
	\item We have simplified the contents about the definition of \rgmapping. (R2-i.6, R3-O1.1, R3-M1, R3-C1, M-C7)
	\item We have added an ER diagram for the Person, Knows, Likes, and Message tables to Figure 2 (Figure 1 in the original paper), and have included corresponding descriptions in the main text to explain the relationships between vertex relations and edge relations in E-R terms. (R3-O1.1)
	\item We have enlarged the font sizes in Figure 2 (Figure 1 in the original manuscript) from 28pt to 32pt. (R1-D1)
\end{itemize}

\textbf{In Section 3, we mainly make the following modifications}:
\begin{itemize}
	\item We have enlarged the font sizes in Figure 3 (Figure 2 in the original manuscript) from 24pt to 28pt. (R1-D1)
	\item We add more contents in Figure 3 to show what is given up when using the tree decomposition approach in \name. (R2-ii.1, M-C4)
	\item We add several sentences to discuss the differences between tree decomposition applied in this paper and the techniques used by EmptyHeaded and CLFTJ. (R2-i.1, M-C2)
	\item We add a new subsection (i.e., Section 3.1.4) to compare the optimization of \name and Calcite and add two new figures, i.e.~Figure 4(b) and Figure 4(c), to show the experimental results. (R2-S1, R2-ii.1, M-C6)
	\item We add a sentence to emphasize that the intersection evaluation using worst-case optimal join is implementaed on relational tables. (R2-i.5)
\end{itemize}

\textbf{In Section 4, we make the following modifications}:
\begin{itemize}
	\item We have enlarged the font sizes in Figure 6 (Figure 5 in the original manuscript) from 28pt to 32pt. (R1-D1)
	\item We add a new paragraph to explain why HASH\_JOIN is used for the entire plan in the absence of a graph index. (R3-O2.5)
	\item We explain why we limit the extensive depth, methods, and literature on relational query optimization. (R3-O4.1, R3-C3)
	\item We add a new paragraph to explain that \filterrule is a global optimization while \fusionrule is a local optimization rule. Besides, we briefly describe two more optimization rules we applied in \name. (R2-i.3)
	\item We introduce the higher-order graph statistics in more detail. (R1-O1, M-C5)
\end{itemize}

\textbf{In Section 5, we make the following modifications}:
\begin{itemize}
	\item We add two new baselines, i.e., Umbra and K\`uzu, add description about them, and conduct compreshensive experiments on them. The experimental results are shown in the revision and compared with \name. (R2-i.4, R2-iii.1, R2-C1, R2-C3, R3-O2.3, R3-C2, M-C1, M-C3). 
	\item We conduct new compreshensive experiments on a larger instance, i.e., LDBC100 to make the conclusions more convincing. Besides, we add the statistics of LDBC100 to Table 2. (R3-O2.6, R3-C2)
	\item We have add some explanations about the procedures for data loading and graph index construction in thie revision. (R3-O2.2)
	\item We add the description about the versions of DuckDB used in the experiments. (R2-iii.2)
	\item We have fixed the typo in this revision and modified the size of the RAM to 256GB.
	\item We enlarge the font sizes in Figures 7--11 (Figures 6--10 in the original manuscript) from 20/22pt to 26/28pt. (R1-D1)
	\item We have redrawn Figure 11 (Figure 10 in the original manuscript), normalized all runtimes to that of DuckDb, and plot speedups achieved by \name and the baselines. (R1-M2)
	\item We add two paragraphs to discuss how to extend \name to deal with queries wihout explicit PGQ component. Besides, we explain about the potential tradeoffs between global and local optimizations (R2-i.3, M-C6, R3-O3.1, R3-C3)
	\item We add a new section, i.e., Section 5.4 (Case Study), and a new figure (i.e., Figure 13) to demonstrate why plans generated by \name are better than those generated by the baselines. (R1-O2, M-C4)
	\item We provide the query statement for a representative SQL/PGQ query (i.e. $JOB_{17}$) in Figure 12 in Section 5.4. (R3-O2.1, M-C4)
\end{itemize}

\textbf{In Section 6, we make the following modifications}:
\begin{itemize}
	\item We add the citations of the related work mentioned by Reviewer \#2 in this section and describe these works briefly. (R2-C2)
	\item We explain the relationships between DuckPGQ and DuckDB in more detail in this revision. (R2-i.2, R2-C2, M-C2)
\end{itemize}

Please note that we find that K\`uzu is slower than \name on both the LDBC and JOB benchmarks, even slower than DuckDB. We have raised an issue on their GitHub to inquire about this problem \footnote{https://github.com/kuzudb/kuzu/issues/3865}, but as of now, they have not responded. Therefore, we reported our experimental results in the manuscript.

\section{Detailed Response to the Feedbacks}
\label{sec:response}
\subsection{Response to Reviewer \#1}

\textbf{O1. I did not understand from the discussion in section 4.2 what are all types of statistics that are maintained to support the optimization. Does the converged approach work with common types of statistics used on relational data or some adjustments were necessary? Did GLogue data structure also require adjustments compared to the graph native design? I would appreciate more details in this area of the design, especially around the higher-order graph statistics.}

%\begin{mdframed}[linewidth=0.5pt, linecolor=black]
\textbf{\textit{Response: }}
Thank you for your suggestion! We add detailed descriptions about the higher-order graph statistics and explain the relationship between the statistics used in this work and the low-order statistics used in previous works in Section 4.3.
%\end{mdframed}



\textbf{O2. It is not clear to me what are typical shapes of the plans that lead to such speedups. It would be nice to include representative examples of more efficient plans.}

\textbf{\textit{Response: }}
We add a case study in Section 5.4, present and compare the plans optimized by \name, GRainDB and Umbra. 
The plans are shown in a newly added figure (i.e., Figure 12). 
Besides, we analyze the reason for the efficiency of the plans generated by \name.


\textbf{D1. Many aspects of the presentation of figures can be improved: 
(1) Workflow illustrations on figures 1,2 and 5 have tiny graphs
(2) Figures 6--10 showing experimental results use tiny fonts.}

\textbf{\textit{Response: }}
We have enlarged the font sizes in the mentioned figures and make the figures larger when possible.
For example, the font sizes in Figure 6 are increased from 24/28pt to 28/32pt.


\textbf{M1. As DRAM nowadays comes in sizes that are multiple of 32GB, it is quite unusual to have 251GB. Is this a typo?}

\textbf{\textit{Response: }}
We have fixed the type in this revision and modified the size of the RAM to 256GB in Section 5.1.

\textbf{M2. Please consider normalizing all runtimes to one system, perhaps DuckDB, and plotting speedups achieved by the other two in figure 10.}

\textbf{\textit{Response: }}
We have redrawn Figure 11 (Figure 10 in the original manuscript), normalized all runtimes to that of DuckDB, and plotted the speedups achieved by \name and the baselines.


\subsection{Response to Reviewer \#2}

\textbf{S1. However given how read-heavy the queries are, I wonder if the query optimization time matters in practice.}

\textbf{\textit{Response: }}
Thanks for your suggestion! We add a new subsection (i.e., Section 3.1.4) to compare the optimization time of \name and Calcite on LDBC queries.
In detail, we add a new figure, i.e. Figure 4(b), to show the experimental results.
The results illustrate that it is important to reduce the time of query optimization in practical applications. 



\textbf{
i.1. I think it is important to mention the differences that exist between the techniques used by EmptyHeaded [r3] and CTJ [r4]  and tree decomposition in this manuscript.}

\textbf{\textit{Response: }}
We have added a discussion about the differences between these techniques and RelGo in more detail in Section 3.1.2.


\textbf{i.2. How do you contrast your approach with the query evaluation where the DuckDB team introduces SQL/PGQ through the extension framework in [40] and [41].}

\textbf{\textit{Response: }}
We explain the relationships between DuckPGQ and DuckDB in Section 6 and add a sentence to emphasize the differences between DuckPGQ and \name.


\textbf{i.3. filter push-down, called `FilterIntoMatchRule` is an artifact of separating the evaluation into a matching operator instead of a global optimization approach and is seen as necessity in my opinion and not a new optimization technique.}

\textbf{\textit{Response: }} We add a new paragraph in Section 4.2.3 to state that \filterrule is a global optimization while \fusionrule is a local optimization rule.
However, because \filterrule is generally effective, we implement it as a local optimization for better optimization performance.
Due to the length limitation, the reason why \filterrule is a global optimization will be detailed in the full version of this manuscript.


\textbf{
i.4. We need a comparison to Umbra and perhaps making it apple-to-apple means obtaining the plan from Umbra and hardcoding it to run within your DuckDB implementation.
}

\textbf{\textit{Response: }} In this revision, we have added two new baselines, i.e., Umbra and K\`uzu.
Specifically, we add descriptions about them in Section 5.1 and conduct compreshensive experiments on them.
The experimental results are shown in Section 5.3 and Figure 11.
In the experiments, the plan optimized by Umbra are hardcoded to plans executable by DuckDB, while K\`uzu is tested on its own execution engine because it is a graph database management system.
The experimental results are analyzed in the newly added subsections (i.e., Section 5.3.2 and Section 5.3.3).


\textbf{
i.5. The intersection evaluation using WCOJ is very similar to that in [34] and it is hard to tell the difference. My understanding is there is no relational tables in that case.}

\textbf{\textit{Response: }}
We have added a sentence in Section 3.2.2 to emphasize that the intersection evaluation using worst-case-optimal join in \name is implemented on relational tables, which is the main difference between our method and HUGE.


\textbf{
i.6. Most of the mapping work is assumed based on the SQL spec and is in use already as per [40] and [41] which is why I do not see it as a contribution. Cypher-based systems like Kuzu do this mapping in their DDL as well and are able to query from relational stores directly.}

\textbf{\textit{Response: }}
We have removed \rgmapping from the contributions of this paper.
Besides, we add K\`uzu as a new baseline of this manuscript and the experimental results are shown and analyzed in Section 5.3.


\textbf{
ii.1 verifying that truly this notion of graph-aware optimization is what lead to the gains requires a clear discussion of i) and further highlighting important design goals that make the smaller search space aspect appealing.
It is unclear what is given up when using the tree decomposition approach proposed.}

\textbf{\textit{Response: }}
We add a new subsection (i.e., Section 3.1.4) to compare the optimization time of \name and Calcite on LDBC queries.
In detail, we add a new figure, i.e. Figure 4(b), to show the experimental results.
The results illustrate that it is important to reduce the time of query optimization in practical applications. 
imization in practical applications. 

Moreover, we add more contents to Figure 3 to show what is given up when using the tree decomposition approach in \name.


\textbf{
iii.1 using DuckDB as a baseline can be misleading in the results
as DuckDB doesn't not contain a state of the art query optimizer, the focus of this paper, and does not support worst-case optimal joins.}

\textbf{\textit{Response: }}
The response to this comment is similar to that of i.4.
Please note that the added new baselines, i.e., i.e., Umbra and K\`uzu, both support worst-case optimal joins.


\textbf{
iii.2 DuckDB and GrainDB are possibly of different versions and so is the authors implementation. This level of detail is not reflected in the experimental section.}

\textbf{\textit{Response: }}
We have added some descriptions to Section 5.1 to clarify that DuckDB v0.9.2 is used in this manuscript and we have reimplemented GRainDB on DuckDB for fair comparison.


\textbf{
C1. Generate Umbra plans, hardcode these plans within the authors solution using their operators and run the plan (multi-way joins) should use the adj. list intersection. Compare the plan with that generated by your system. }

\textbf{\textit{Response: }}
The response to this comment is similar to that of i.4.

\textbf{
C2. Add some of the work that I mention as missing to the related work section.}
% with the goal of explaining how tree decomposition in your approach defers from that of EmptyHeaded. }

\textbf{\textit{Response: }} We add the related works you mentioned to Section 6 in the revision. Besides, we analyze the difference between the tree decomposition in \name with those of EmptyHeaded[r3] and CLFTJ[r4] in more detail in Section 3.1.2.


\textbf{
C3. Getting rough numbers from Umbra and Kuzu as sanity checks would be nice to have but not necessary.}
% Umbra shows that this system does move things forward when compared with a pure RDBMS. Kuzu would potentially show either that such systems have less value as SQL/PGQ is adopted or that it has certain optimizations or plans that your system or RDBMSs more generally cannot generate. }

\textbf{\textit{Response: }}
The response to this comment is similar to that of i.4.


\subsection{Response to Reviewer \#3}

\textbf{
O1.1 The explication misses some seemingly rather important points.
For instance, zeta for vertex and edge mappings? These are introduced, but then never used again. But really, the authors spend much realestate to formalize the SQL/PGM mapping, when that is not a contribution of the paper.}

\textbf{\textit{Response: }}
Thank you for your comment! We have simplified the definition of \rgmapping for better understanding and refined the descriptions about the total functions mapping from edge relations to vertex relations in Example 2.
Besides, we remove the notations ``zeta for vertex and edge mappings'' that never used in this manuscript.


\textbf{
O1.2 There is no guaranteed bijection here. The relational model does not use object IDs; graph data models do. The introduction of UUIDs skirts the issue, to be replaced shortly later in the paper with row-IDs. It seems rather that one would want to insist that any table manifested as an labeled edge set via GRAPH\_TABLE have foreign keys to both the node sets / entity tables it is being used to bridge. This could be conveyed more simply perhaps in E-R terms.}

\textbf{\textit{Response: }}
We add a new sentence about the mapping in Section 2.1, i.e., ``These total functions are typically established through primary-foreign key relationships, which are best illustrated in an Entity-Relationship (ER) diagram''.
Besides, we add an ER diagram in Figure 2 corresponding to example provided in Example 2, to help understand the primary-foreign key relationships between relations. 


\textbf{O2.1 No concrete insight is offered into the test queries, nor concrete examples. }

\textbf{\textit{Response: }}
We provide the query statement for a representative SQL/PGQ query (i.e., $JOB_{17}$) in Figure 12(a) as an example.
Besides, we add a new section (i.e., Section 5.4) to present a case study that compares the plans for this query optimized by \name, GRainDB and Umbra.
We also analyze the reasons why the plan optimized by \name achieved better performance in Section 5.4.


\textbf{
O2.2 Not everyone will be familiar with [21]. The paper needs to be self-contained. }

\textbf{\textit{Response: }}
We add an explanation about the method to construct the graph index in Section 5.1.

\textbf{ 
O2.3 I recommend that at least an additional relational engine be included, perhaps PostgreSQL, in the comparisons. The authors extended a front-end to DuckDB to handle SQL/PGQ queries, although I do not understand why. Instead, having pairs in the test suite of SQL and SQL/PGO that are semantically equivalent but manually composed I believe would be fairer. And these could be used, say, for PostgreSQL without needing to reinstrument it.}

\textbf{\textit{Response: }}
To make the conclusions more convincing, we have added two new baselines in our experiments, i.e., Umbra and K\`uzu.
Umbra is a relational database that supports worst-case optimal joins and K\`uzu is a baseline of a graph database management system (GDBMS).
Specifically, we add descriptions about them in Section 5.1 and conduct compreshensive experiments on them.
The experimental results are shown in Section 5.3 and Figure 11.
In the experiments, the plan optimized by Umbra are hardcoded to plans executable by DuckDB, while K\`uzu is tested on its own execution engine because it is a graph database management system.
The experimental results are analyzed in the newly added subsections (i.e., Section 5.3.2 and Section 5.3.3).


\textbf{
O2.4 A concrete example of a "converged" query plan would be exceedingly helpful.} 

\textbf{\textit{Response: }} 
The response to this comment is similar to that of O2.1.
Please note that the plan generated by \name shown in Figure 12(b) us an example of a converged query plan.

\textbf{
O2.5 And a less simplistic discussion of the plans and operators. For example, at "In the absence of a graph index, HASH\_JOIN is used for the entire plan, where the cost is simply the product of the cardinalities of the two relations being joined," I lost the narrative somewhat. }

\textbf{\textit{Response: }}
We have added a new paragraph in Section 4.2.1 to explain why HASH\_JOIN is used for the entire plan in the absence of a graph index.


\textbf{
O2.6 The paper could be strengthened by adding significantly larger data instances.}

\textbf{\textit{Response: }}
In this revision, we have conducted comprehensive experiments on a much larger data instance, i.e., $LDBC100$, which contains 282 million tuples in vertex relations and 938 million tuples in edge relations.
The experimental results are shown in Figure 11(a) and analyzed in Section 5.3.


\textbf{O3.1 SQL/PGQ goes against the declarative ideal of "state what you want, not how to do it." 
Could RelGo be extended to additionally rewrite SQL/PGQ queries, and SQL queries with no explicit PGO component in some cases, to consider "SPJR" query plans? 
Providing the discussion and a roadmap for doing so would greatly strengthen the contributions.}

\textbf{\textit{Response: }}
It is an interesting idea and we add a new paragraph dicussing about this in Section 7.

\textbf{O4.1 The authors short the extensive depth, methods, and literature on relational query optimization. While the authors are constrained by space and focus by the conference paper format, explanation of how and why they limit their focus in the ways they do should be offered. }

\textbf{\textit{Response: }}
We add some explanations in Section 4.2.2 about why we do not talk much about relational query optimization.
Besides, we state that we focus on how to enhance relational queries with graph optimization techniques. 


\textbf{M1. I would recommend moving Table 1 earlier; and also simplifying the needed notation. I think the query example on page 1 would be better as a figure float. }

\textbf{\textit{Response: }}
In this revision, we mention Table 1 at the beginning of Section 2.
Besides, we have simplified the definition of \rgmapping in Section 2.1 and made the query example on Page 1 a figure float. 


\textbf{
C1. My O1 and O2. The paper does not convey enough in concrete terms how their approach works overall that the reader can take concrete knowledge away from it. The abundance of the formal discussions of SQL/PGO can be traded to provide more in depth, concrete insight into RelGo.}

\textbf{\textit{Response: }}
We have simplified the formal discussions of SQL/PGQ and focus more on our contributions.
Our main revisions about this comment have been presented in our responses to O1.1--O2.6.


\textbf{
C2. I think additional experiments are needed, as I outline in O2. If not, then the authors should make a compelling case in discussion in the paper as to why they are not needed.}

\textbf{\textit{Response: }}
We have added more experiments with two more baselines (i.e., Umbra and K\`uzu) and a larger instance (i.e., $LDBC100$) in Section 5 of this revision.
Besides, we have added a new section (i.e., Section 5.4) to provide a case study and  demonstrate the advantages of the plans optimized by \name more intuitively.


\textbf{
C3. Addressing O3 \& O4 to some extent would strengthen the paper, I believe.}

\textbf{\textit{Response: }}
We have modified this revision according to O3 \& O4, our detailed revisions can be found in our responses to O3.1 and O4.1.



\subsection{Response to the Meta Reviewer}

\textbf{
C1. Comparison with prior work in particular Umbra emerged as an important consideration among other required improvements listed below.}

\textbf{\textit{Response: }}
Thanks for your suggestion! 
In this revision, we have added two new baselines, i.e., Umbra and K\`uzu.
Specifically, we add descriptions about them in Section 5.1 and conduct compreshensive experiments on them.
The experimental results are shown in Section 5.3 and Figure 11.
In the experiments, the plan optimized by Umbra are hardcoded to plans executable by DuckDB, while K\`uzu is tested on its own execution engine because it is a graph database management system.
The experimental results are analyzed in the newly added subsections (i.e., Section 5.3.2 and Section 5.3.3).



\textbf{
C2. Significant relevant related work is not discussed, compared, and contrasted. [R2]
a. Add reference, discussion, and comparison with [r1–r5] from R2a-iii.
}

\textbf{\textit{Response: }}
In the revision, we have added the references to [r1--r5] from R2a-iii and described these works in Section 6.
Besides, we have made some discussions and comparisons w.r.t.~EmptyHeaded[r3] and CLFTJ[r4] in Section 3.1.2.


\textbf{C2. b. Add experimental evaluation comparing against Umbra and Kùzu. Redo the experiments with larger instances.}

\textbf{\textit{Response: }}
In this revision, we have added Umbra and K\`uzu as two new baselines and conduct experiments on both the LDBC SNB and JOB benchmarks (as explained in our response to C1).


Besides, we redo the comprehensive experiments on a larger instance, i.e., $LDBC100$, which contains 282 million tuples in vertex relations and 938 million tuples in edge relations.
The experimental results are presented and analyzed in Section 5.3.


\textbf{
C3. 2. More concrete discussion and explanation about the optimization platform and in the experiments. \\
a. Add concrete examples of queries and query plans used in the experiments. [R1-O2, R3-O2]
}

\textbf{\textit{Response: }}
We provide the query statement for a representative SQL/PGQ query (i.e., $JOB_{17}$) in Figure 12(a) as an example.
Besides, we add a new section (i.e., Section 5.4) to present a case study that compares the plans for this query optimized by \name, GRainDB and Umbra.
We also analyze the reasons why the plan optimized by \name achieved better performance in Section 5.4.


\textbf{
C4. b. State clearly “all types of statistics that are maintained to support the optimization.” [R1-O1]}

\textbf{\textit{Response: }}
We add detailed descriptions about the higher-order graph statistics and explain the relationship between the statistics used in this work and the low-order statistics used in previous works in Section 4.3.


\textbf{
C5. c. Be much clearer in discussion about the tradeoffs being made here between local and global optimization. And that involved in restricting to a smaller search space. Show that the optimization time is relevant in practice.}

\textbf{\textit{Response: }} 
In this revision, we add a new paragraph in Section 4.2.3 to explain that \filterrule is a global optimization while \fusionrule is a local optimization rule.
However, because \filterrule is generally effective, we implement it as a local optimization for better optimization performance.

Besides, we add a new subsection (i.e., Section 3.1.4) to compare the optimization time of \name and Calcite on LDBC queries.
In detail, we add a new figure, i.e. Figure 4(b), to show the experimental results.
The results illustrate that it is important to reduce the time of query optimization in practical applications. 


\textbf{
C6. 3. Rewrite the presentation of SQL/PL. [R2-i; R3-O1]
}

\textbf{\textit{Response: }}
In this revision, we have simplified the definition of \rgmapping in Section 2.1 and remove the notations ``zeta for vertex and edge mappings'' that never used in this manuscript.
Besides, we add a sentence about the total functions between edge relations and vertex relations in Section 2.1, i.e., ``These total functions are typically established through primary-foreign key relationships, which are best illustrated in an Entity-Relationship (ER) diagram''.
Moreover, we add an ER diagram in Figure 2 corresponding to the example provided in Example 2 to help understand the primary-foreign key relationships between relations.