\section{Related Work}
\label{sec:related-work}
%\textit{GetVertex}, \textit{GetEdge}, and \textit{GetNeighbor} are commonly used in graph queries.
%Specifically, \textit{GetVertex} is used to get the adjacent vertices of an edge, \textit{GetEdge} returns the edges from or to a given vertex, and \textit{GetNeigbor} gets the neighbors of a given vertex.
%Based on SQL/PGQ, in relational database, these operators can be implemented by joining tables representing vertices and edges.
%Then, it is crucial to support efficient join order optimization in the converged graph relational optimizer.

%In this section, we survey the query optimization techniques for relational databases and graph databases, respectively.
%Moreover, the mapping between relational data model and property graph
%Moreover, some relational databases (e.g., Oracle) allow users to create graph indices on relational tables.
%Then, researchers study to improve the performances of query execution with such graph indices.
%Therefore, this kind of works are also included in this section.
%These two types of works are also included in this section.

\stitle{Query Optimization for Relational Databases.} Join order optimization was a traditional and crucial topic for query optimization in relational databases. The order in which joins were conducted could influence the execution time significantly and had attained substantial accomplishments. Various studies of query optimization for relational databases were proposed to find the optimal join order. Ibaraki et al.\cite{nested-tods-1984} proposed that there were usually fewer than ten tables involved in a typical query, and dealt with joins using nested-loops join. Specifically, they proved the NP-complexity of the join order optimization problem and designed an efficient algorithm with a time complexity of $O(n^2logn)$ to optimize tree queries. Then, Krishnamurthy et al.\cite{optimize-nested-vldb-1986} optimized the algorithm and proposed an algorithm with a time complexity of $O(N^2)$ by reusing the computation results. Besides, the authors emphasized that since finding the optimal join order was a complex problem, it was more important to avoid the worst plan. Moreover, Haffner et al.\cite{astarjoin} converted the problem of join order optimization to that of finding the shortest path on directed graphs and solved the problem with the A* algorithm. In detail, four heuristics were designed to estimate the remaining cost. Furthermore, Kossmann et al.\cite{data-dependency-join} summarized the methods to optimize queries with data dependencies, such as uniqueness constraints, foreign key constraints, and inclusion dependencies.
\comment{
For example, given two tables $T_1$ and $T_2$, if the values of attribute $A_1$ on $T_1$ are unique, then the inner-join in queries like
\begin{lstlisting}
    SELECT T2.A2 FROM T1, T2
    WHERE T1.A1 = T2.A2;
\end{lstlisting}
may be converted into a semi-join, which is more efficient.
}


Several studies have focused on improving cardinality estimation for cost computation, enabling cost-based optimizers to search for execution plans with potentially lower costs. A common approach is to estimate cardinality through sampling \cite{index-based-join-sampling,ripple-join,wanderjoin,index-based-join-sampling}. For instance, Li et al.\cite{wanderjoin} present an unbiased estimator based on random walks to estimate cardinalities. Leis et al.\cite{index-based-join-sampling} propose a computationally inexpensive method called index-based join sampling to improve accuracy. Other studies estimate cardinalities using histograms \cite{histogram,postgres-row-estimation}. These methods divide the values of a table column into several buckets to build a histogram and assume that the values are uniformly distributed within each bucket. The cardinality is then estimated by summing the number of values in the relevant buckets. Additionally, learning-based methods have been explored for cardinality estimation \cite{learning-based-estimation-1,learning-based-estimation-2,learning-based-estimation-3,learning-based-estimation-4}.

All relational optimizations can be orthogonally adopted in \name's relational optimizer.
\comment{
Some researchers \cite{selinger,postgres-row-estimation} estimate the number of cardinalities by computing the selectivity of $A \bowtie_{A.col_1 = B.col_2} B$ as
\begin{equation*}
    \frac{1}{max(DV(A.col_1), DV(B.col_2))},
\end{equation*}
where $DV(A.col_1)$ is the number of distinct values of $A.col_1$ in table $A$.
}


\stitle{Query Optimization for Graph Databases.} Graph pattern matching is a fundamental problem in graph query processing and has been widely studied~\cite{angles2017foundations}. In sequential settings, Ullmann's traversal-based backtracking algorithm~\cite{ullmann1976algorithm} paved the way for various optimizations, such as tree indexing\cite{shang2008quicksi}, symmetry breaking~\cite{han13turbo}, and compression techniques~\cite{bi2016efficient}. However, due to the difficulties in parallelizing backtracking algorithms, join-based algorithms have been developed in distributed environments. These algorithms rely on cost estimation to optimize join order for efficient pattern matching. Binary-join algorithms\cite{lai2015scalable, lai2019distributed} estimate costs using random graph models, while worst-case-optimal join algorithms~\cite{ammar2018distributed} ensure that the cost remains within a worst-case upper bound. Hybrid approaches~\cite{mhedhbi2019optimizing, huge} have been proposed to adaptively select between binary and worst-case optimal join techniques based on which method incurs lower costs in a given situation. Recent studies have focused on enhancing cost estimation in graph pattern matching to achieve better matching orders and execution plans. These include C-Set \cite{cset} that decomposes the query and data graphs into star-shaped subgraphs and AutoMine \cite{AutoMine} that estimates costs based on the number of iterations in nested loops. Some optimizers, such as GLogS~\cite{GLogS}, search for the optimal plan by representing edges as binary joins or subtasks of extensions and computing the optimal plan in a bottom-up manner to obtain a worst-case optimal plan efficiently. Additionally, researchers in~\cite{gcare} have compared the performance of different cardinality estimation methods for graph pattern matching.

We adopt join-based methods similar to those in~\cite{huge,GLogS} due to their compatibility with the relational context for which \name~ is designed. However, to ensure seamless integration with the relational environment, we need to make adaptations to these methods accordingly.


\stitle{Bridging Relational and Graph Models.} Due to the semantic equivalence between relational and graph models, there is a growing interest in the interaction between these two data models. Index-based methods, such as GQ-Fast \cite{gqfast} and GrainDB \cite{graindb}, work with traditional relational databases without the need for graph materialization. GQ-Fast focuses on relationship queries and generates physical plans by transforming relationship query normalized algebra expressions, while GrainDB constructs RID indices and introduces predefined join methods to efficiently retrieve adjacent edges and vertices. These techniques contribute to graph index construction, which can be leveraged by \name to compute execution plans involving efficient graph-related physical operations. In contrast, methods like GRFusion \cite{GRFusion} and Gart~\cite{gart} rely on graph materialization, which incurs additional storage costs and potential inconsistencies between relational and graph data. GRFusion builds an in-memory graph view based on relational tables and supports queries on both tables and graph views. This allows some relational joins to be replaced with graph traversals, enabling the application of graph-level optimizations. Similarly, in the work of Gart, the authors develop ETL tools to convert and import relational data into a graph store, upon which any graph processing system can be plugged in for efficient graph analysis. We prefer index-based techniques over materialization techniques due to their ability to work directly with relational databases, avoid additional storage costs, and ensure consistency between relational and graph data, making them more suitable for the relational context in which \name operates.



%In terms of translation-based methods, Apache/Age \cite{apache-age} is a typical work.
%Apache/Age is an extension of PostgreSQL, and it provides the ability to handle hybrid queries including openCypher and SQL.
%In detail, after a graph is created, a namespace with the same name as the graph is created.
%The vertices and edges in the graph are stored in corresponding tables in the namespace.
%When a query with both openCypher and SQL statements is performed, Apache/Age transforms the openCypher statements and converts the operators to those in PostgreSQL (e.g., MATCH PATH $\rightarrow$ join), and then the query is solved by PostgreSQL.
%It seems that Apache/Age is more like a syntactic sugar, and the advantanges of graphs and graph optimizers are not utilized.

%In terms of index-based methods, these methods build graph indices on relational databases, and attempt to accelerate queries with the indices.


